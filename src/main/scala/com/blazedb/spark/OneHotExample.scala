package com.blazedb.spark

import org.apache.spark.sql.functions.{col, lit}
import org.apache.spark.sql.types.DoubleType
import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer, VectorAssembler}
import org.apache.spark.ml.Pipeline
import org.apache.spark.mllib.linalg.Vector
import org.apache.spark.sql.{DataFrame, SQLContext}


object OneHotExample {

  /*
  Here are 2 code snippets:
(1) Compute one-hot encoded data for Spark, using the data generated by https://github.com/szilard/benchm-ml/blob/master/0-init/2-gendata.txt
(2) Run MLlib, computing soft predictions by hand.

I ran these with Spark 1.4, and they should work for 1.5 as well.

Note: There's no real need to switch to DataFrames yet for benchmarking.  Both the RDD and DataFrame APIs use the same underlying implementation.  (I hope to improve on that in Spark 1.6 if there is time.)

Ran on EC2 cluster with 4 workers with 9.6GB memory each, and 8 partitions for training RDD.
For the 1M dataset, training the forest took 2080.814977193 sec and achieved AUC 0.7129779357732448 on the test set.

(1) Code for one-hot encoding

*/


  // Paths
  val origDataDir = "/mnt/mllib/regression/flightTimes/prepped"
  val origTrainPath = origDataDir + "/train-10m.csv"
  val origTestPath = origDataDir + "/test.csv"
  val newDataDir = "/mnt/mllib/regression/flightTimes/spark"
  val newTrainPath = newDataDir + "/spark-train-10m.FIXED.parquet"
  val newTestPath = newDataDir + "/spark-test.FIXED.parquet"

  def display(df: DataFrame) = df.show(50,false)

  def oneHot(sqlContext: SQLContext) = {
    // Read CSV as Spark DataFrames
    val trainDF = sqlContext.read.format("com.databricks.spark.csv").option("header", "true").load(origTrainPath)
    val testDF = sqlContext.read.format("com.databricks.spark.csv").option("header", "true").load(origTestPath)

    // Combine train, test temporarily
    val fullDF = trainDF.withColumn("isTrain", lit(true)).unionAll(testDF.withColumn("isTrain", lit(false)))
    display(fullDF)

    // Feature types
    val vars_categ = Array("Month", "DayofMonth", "DayOfWeek", "UniqueCarrier", "Origin", "Dest")
    val vars_num = Array("DepTime", "Distance")
    val vars_num_double = vars_num.map(_ + "_double")
    val var_y = "dep_delayed_15min"

    // Cast column types as needed
    val fullDF2 = fullDF.withColumn("DepTime_double", col("DepTime").cast(DoubleType)).withColumn("Distance_double", col("Distance").cast(DoubleType))
    display(fullDF2)

    // Assemble Pipeline for featurization.
    // Need to use StringIndexer for OneHotEncoder since it does not yet support String input (but it will).
    val stringIndexers = vars_categ.map(colName => new StringIndexer().setInputCol(colName).setOutputCol(colName + "_indexed"))
    val oneHotEncoders = vars_categ.map(colName => new OneHotEncoder().setInputCol(colName + "_indexed").setOutputCol(colName + "_ohe").setDropLast(false))
    val catAssembler = new VectorAssembler().setInputCols(vars_categ.map(_ + "_ohe")).setOutputCol("catFeatures")
    val featureAssembler = new VectorAssembler().setInputCols(vars_num_double :+ "catFeatures").setOutputCol("features")
    val labelIndexer = new StringIndexer().setInputCol(var_y).setOutputCol("label")
    val pipeline = new Pipeline().setStages(stringIndexers ++ oneHotEncoders ++ Array(catAssembler, featureAssembler, labelIndexer))

    // Compute features.
    val pipelineModel = pipeline.fit(fullDF2)
    val transformedDF = pipelineModel.transform(fullDF2)
    display(transformedDF)

    // Split back into train, test
    val finalTrainDF = transformedDF.where(col("isTrain"))
    val finalTestDF = transformedDF.where(!col("isTrain"))

    // Save Spark DataFrames as Parquet
    finalTrainDF.write.mode("overwrite").parquet(newTrainPath)
    finalTestDF.write.mode("overwrite").parquet(newTestPath)

  }

}
